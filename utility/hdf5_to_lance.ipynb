{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d861cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428c490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path = \"/bd_byta6000i0/users/surgical_depth/SCARED_fixed/scared.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81544ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file =  h5py.File(h5_path, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8bd20cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['dataset1', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38254d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773f2a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['keyframe1', 'keyframe2', 'keyframe3']>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file['dataset1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2c8c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['000001', '000002', '000003', '000004', '000005', '000006', '000007', '000008', '000009', '000010', '000011', '000012', '000013', '000014', '000015', '000016', '000017', '000018', '000019', '000020', '000021', '000022', '000023', '000024', '000025', '000026', '000027', '000028', '000029', '000030', '000031', '000032', '000033', '000034', '000035', '000036', '000037', '000038', '000039', '000040', '000041', '000042', '000043', '000044', '000045', '000046', '000047', '000048', '000049', '000050', '000051', '000052', '000053', '000054', '000055', '000056', '000057', '000058', '000059', '000060', '000061', '000062', '000063', '000064', '000065', '000066', '000067', '000068', '000069', '000070', '000071', '000072', '000073', '000074', '000075', '000076', '000077', '000078', '000079', '000080', '000081', '000082', '000083', '000084', '000085', '000086', '000087', '000088', '000089', '000090', '000091', '000092', '000093', '000094', '000095', '000096', '000097', '000098', '000099', '000100', '000101', '000102', '000103', '000104', '000105', '000106', '000107', '000108', '000109', '000110', '000111', '000112', '000113', '000114', '000115', '000116', '000117', '000118', '000119', '000120', '000121', '000122', '000123', '000124', '000125', '000126', '000127', '000128', '000129', '000130', '000131', '000132', '000133', '000134', '000135', '000136', '000137', '000138', '000139', '000140', '000141', '000142', '000143', '000144', '000145', '000146', '000147', '000148', '000149', '000150', '000151', '000152', '000153', '000154', '000155', '000156', '000157', '000158', '000159', '000160', '000161', '000162', '000163', '000164', '000165', '000166', '000167', '000168', '000169', '000170', '000171', '000172', '000173', '000174', '000175', '000176', '000177', '000178', '000179', '000180', '000181', '000182', '000183', '000184', '000185', '000186', '000187', '000188', '000189', '000190', '000191', '000192', '000193', '000194', '000195', '000196', '000197']>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file['dataset1']['keyframe1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ead6058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['gt', 'image']>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file['dataset1']['keyframe1']['000001'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3298148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"gt\": shape (1024, 1280, 1), type \"<f4\">"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file['dataset1']['keyframe1']['000001']['gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c592b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(h5file['dataset1']['keyframe1']['000001']['image'].compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d111bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import lance\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb891e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在扫描 HDF5 索引 (为了物理打乱数据)...\n",
      "扫描完成，共找到 28857 帧。正在打乱顺序...\n",
      "\n",
      "目标路径: /bd_byta6000i0/users/surgicaldinov2/kyyang/surgicaldinov3/data/SCARED_hdd.lance\n",
      "策略: Batch Size=50, Group Size=64\n",
      "开始读取数据并写入...\n",
      "Processing 0/28857...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[\u001b[0m2025-12-11T09:50:29Z \u001b[33mWARN \u001b[0m lance::dataset::write::insert\u001b[90m]\u001b[0m No existing dataset at /bd_byta6000i0/users/surgicaldinov2/kyyang/surgicaldinov3/data/SCARED_hdd.lance, it will be created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 28800/28857...\n",
      "写入全部完成！\n",
      "------------------------------\n",
      "验证读取性能...\n",
      "总行数: 28857\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 161\u001b[39m\n\u001b[32m    158\u001b[39m depth = item[\u001b[33m'\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(depth, \u001b[33m'\u001b[39m\u001b[33mto_numpy\u001b[39m\u001b[33m'\u001b[39m): depth = depth.to_numpy()\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage Info: Shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    162\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDepth Info: Shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    163\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m数据完整性检查通过。\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import lance\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "def convert_scared_hdf5_to_lance_hdd(h5_path, lance_path):\n",
    "    # --- 1. 参数配置 (HDD 优化版) ---\n",
    "    BATCH_SIZE = 50           # 控制内存：防止 OOM\n",
    "    MAX_ROWS_PER_GROUP = 64   # 控制磁盘：HDD 需要更大的连续块 (约 560MB/组)\n",
    "    H, W = 1024, 1280\n",
    "\n",
    "    # 定义 Tensor 类型\n",
    "    # Image: (H, W, 3) uint8\n",
    "    image_tensor_type = pa.fixed_shape_tensor(pa.uint8(), (H, W, 3))\n",
    "    # Depth: (H, W, 1) float32 (保留 Channel 维)\n",
    "    depth_tensor_type = pa.fixed_shape_tensor(pa.float32(), (H, W, 1))\n",
    "\n",
    "    # 定义 Schema\n",
    "    schema = pa.schema([\n",
    "        pa.field(\"dataset_id\", pa.string()),\n",
    "        pa.field(\"keyframe_id\", pa.string()),\n",
    "        pa.field(\"frame_name\", pa.string()),\n",
    "        pa.field(\"image\", image_tensor_type),\n",
    "        pa.field(\"depth\", depth_tensor_type)\n",
    "    ])\n",
    "\n",
    "    # --- 2. 预扫描与打乱 (HDD 核心优化) ---\n",
    "    print(f\"正在扫描 HDF5 索引 (为了物理打乱数据)...\")\n",
    "    all_frames = []\n",
    "    \n",
    "    # 第一次只读 Key，不读数据，速度很快\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        for ds_id in f.keys():\n",
    "            if not isinstance(f[ds_id], h5py.Group): continue\n",
    "            for kf_id in f[ds_id].keys():\n",
    "                if not isinstance(f[ds_id][kf_id], h5py.Group): continue\n",
    "                for frame_name in f[ds_id][kf_id].keys():\n",
    "                    # 检查必要字段是否存在\n",
    "                    grp = f[ds_id][kf_id][frame_name]\n",
    "                    if 'image' in grp and 'gt' in grp:\n",
    "                        all_frames.append( (ds_id, kf_id, frame_name) )\n",
    "\n",
    "    print(f\"扫描完成，共找到 {len(all_frames)} 帧。正在打乱顺序...\")\n",
    "    random.shuffle(all_frames) # <--- 关键：物理乱序写入\n",
    "\n",
    "    # --- 3. 辅助函数：构建 RecordBatch ---\n",
    "    def _create_batch_from_buffer(buffer):\n",
    "        if not buffer[\"dataset_id\"]:\n",
    "            return None\n",
    "\n",
    "        # 构造基础列\n",
    "        arrays = [\n",
    "            pa.array(buffer[\"dataset_id\"]),\n",
    "            pa.array(buffer[\"keyframe_id\"]),\n",
    "            pa.array(buffer[\"frame_name\"]),\n",
    "        ]\n",
    "        names = [\"dataset_id\", \"keyframe_id\", \"frame_name\"]\n",
    "\n",
    "        # 构造 Image Tensor 列\n",
    "        img_stack = np.stack(buffer[\"image\"]) \n",
    "        img_arrow = pa.FixedShapeTensorArray.from_numpy_ndarray(img_stack)\n",
    "        arrays.append(img_arrow)\n",
    "        names.append(\"image\")\n",
    "\n",
    "        # 构造 Depth Tensor 列\n",
    "        depth_stack = np.stack(buffer[\"depth\"])\n",
    "        depth_arrow = pa.FixedShapeTensorArray.from_numpy_ndarray(depth_stack)\n",
    "        arrays.append(depth_arrow)\n",
    "        names.append(\"depth\")\n",
    "\n",
    "        # 修复报错的关键：返回 RecordBatch 而不是 Table\n",
    "        return pa.RecordBatch.from_arrays(arrays, names=names)\n",
    "\n",
    "    # --- 4. 数据生成器 ---\n",
    "    def batch_generator():\n",
    "        buffer = {\n",
    "            \"dataset_id\": [], \"keyframe_id\": [], \"frame_name\": [],\n",
    "            \"image\": [], \"depth\": []\n",
    "        }\n",
    "        \n",
    "        print(f\"开始读取数据并写入...\")\n",
    "        # 保持文件打开状态进行迭代\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            for idx, (ds_id, kf_id, frame_name) in enumerate(all_frames):\n",
    "                if idx % 100 == 0:\n",
    "                    print(f\"Processing {idx}/{len(all_frames)}...\", end='\\r')\n",
    "\n",
    "                # 根据索引定位数据\n",
    "                frame_group = f[ds_id][kf_id][frame_name]\n",
    "\n",
    "                # 读取并处理\n",
    "                img_array = frame_group['image'][:] \n",
    "                \n",
    "                depth_array = frame_group['gt'][:]\n",
    "                # 确保深度图是 (H, W, 1)\n",
    "                if depth_array.ndim == 2:\n",
    "                    depth_array = depth_array[..., np.newaxis]\n",
    "\n",
    "                # 加入缓冲\n",
    "                buffer[\"dataset_id\"].append(ds_id)\n",
    "                buffer[\"keyframe_id\"].append(kf_id)\n",
    "                buffer[\"frame_name\"].append(frame_name)\n",
    "                buffer[\"image\"].append(img_array)\n",
    "                buffer[\"depth\"].append(depth_array.astype(np.float32))\n",
    "\n",
    "                # 缓冲区满，Yield 一个 Batch\n",
    "                if len(buffer[\"dataset_id\"]) >= BATCH_SIZE:\n",
    "                    batch = _create_batch_from_buffer(buffer)\n",
    "                    if batch is not None:\n",
    "                        yield batch\n",
    "                    # 清空 Buffer\n",
    "                    for key in buffer: buffer[key] = []\n",
    "            \n",
    "            # 处理剩余数据\n",
    "            if len(buffer[\"dataset_id\"]) > 0:\n",
    "                batch = _create_batch_from_buffer(buffer)\n",
    "                if batch is not None:\n",
    "                    yield batch\n",
    "\n",
    "    # --- 5. 执行写入 ---\n",
    "    print(f\"\\n目标路径: {lance_path}\")\n",
    "    print(f\"策略: Batch Size={BATCH_SIZE}, Group Size={MAX_ROWS_PER_GROUP}\")\n",
    "    \n",
    "    lance.write_dataset(\n",
    "        batch_generator(),\n",
    "        lance_path,\n",
    "        schema=schema,\n",
    "        mode=\"overwrite\",\n",
    "        max_rows_per_group=MAX_ROWS_PER_GROUP # HDD 优化参数\n",
    "    )\n",
    "    print(\"\\n写入全部完成！\")\n",
    "\n",
    "# --- 验证代码 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置路径\n",
    "    input_h5 = \"/bd_byta6000i0/users/surgical_depth/SCARED_fixed/scared.hdf5\"\n",
    "    output_lance = \"/bd_byta6000i0/users/surgicaldinov2/kyyang/surgicaldinov3/data/SCARED_hdd.lance\"\n",
    "    \n",
    "    if os.path.exists(input_h5):\n",
    "        # 1. 运行转换\n",
    "        convert_scared_hdf5_to_lance_hdd(input_h5, output_lance)\n",
    "        \n",
    "        # 2. 简单验证读取\n",
    "        if os.path.exists(output_lance):\n",
    "            print(\"-\" * 30)\n",
    "            print(\"验证读取性能...\")\n",
    "            ds = lance.dataset(output_lance)\n",
    "            print(f\"总行数: {ds.count_rows()}\")\n",
    "            \n",
    "            # 随机取一行 (因为已经打乱了，take(0) 其实是随机的一张图)\n",
    "            item = ds.take([0], columns=['image', 'depth']).to_pydict()\n",
    "            \n",
    "            img = item['image'][0]\n",
    "            if hasattr(img, 'to_numpy'): img = img.to_numpy()\n",
    "            \n",
    "            depth = item['depth'][0]\n",
    "            if hasattr(depth, 'to_numpy'): depth = depth.to_numpy()\n",
    "\n",
    "            print(f\"Image Info: Shape={img.shape}, Type={img.dtype}\")\n",
    "            print(f\"Depth Info: Shape={depth.shape}, Type={depth.dtype}\")\n",
    "            print(\"数据完整性检查通过。\")\n",
    "    else:\n",
    "        print(f\"找不到输入文件: {input_h5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4d987fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lance = \"/bd_byta6000i0/users/surgicaldinov2/kyyang/surgicaldinov3/data/SCARED_hdd.lance\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b9b7b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "验证读取性能...\n",
      "总行数: 28857\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(output_lance):\n",
    "    print(\"-\" * 30)\n",
    "    print(\"验证读取性能...\")\n",
    "    ds = lance.dataset(output_lance)\n",
    "    print(f\"总行数: {ds.count_rows()}\")\n",
    "    \n",
    "    # 随机取一行 (因为已经打乱了，take(0) 其实是随机的一张图)\n",
    "    batch_table = ds.to_table(columns=[\"image\", \"depth\"], limit=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7164499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "image: extension<arrow.fixed_shape_tensor[value_type=uint8, shape=[1024,1280,3]]>\n",
       "depth: extension<arrow.fixed_shape_tensor[value_type=float, shape=[1024,1280,1]]>\n",
       "----\n",
       "image: [[[102,60,97,104,61,...,7,28,35,7,28],[9,7,5,9,7,...,7,6,11,7,6],...,[51,29,25,54,32,...,9,8,14,9,8],[66,27,28,66,27,...,110,117,125,110,117]]]\n",
       "depth: [[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,57.69668,0,0,0,0],...,[0,78.02751,0,79.12905,78.02449,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98dba1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = ds.take([0], columns=[\"image\", \"depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91062222",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_scalar = tbl[\"image\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25e9ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_scalar = tbl[\"depth\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3348ff47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1280, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_scalar.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c30397a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lance.torch.data\n",
    "\n",
    "# Load lance dataset into a PyTorch IterableDataset.\n",
    "# with only columns \"image\" and \"prompt\".\n",
    "dataset = lance.torch.data.LanceDataset(\n",
    "    \"/bd_byta6000i0/users/surgicaldinov2/kyyang/surgicaldinov3/data/SCARED_hdd.lance\",\n",
    "    columns=[\"image\", 'depth'],\n",
    "    batch_size=6,\n",
    "    batch_readahead=8,  # Control multi-threading reads.\n",
    "    shuffleeee=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89be91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2893b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch[\"image\"].reshape(-1, 1024, 1280, 3).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fbe15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "686fd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d49ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bd_byta6000i0/users/surgicaldinov2/miniforge3/envs/da3/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.v2 as T\n",
    "image_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize(256),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0833033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trans = image_transform(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "907eed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_trans.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19156e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
